# -*- coding: utf-8 -*-
"""fine_tuning_gemma-2b_instruct_for_quotes_jun14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/donbr/505c3101276cfdc5360b2367af9da054/fine_tuning_gemma-2b_instruct_for_summarization_assignment_version_jun14.ipynb

## [Gemma 2B E2E Fine Tuning...](https://colab.research.google.com/gist/donbr/505c3101276cfdc5360b2367af9da054/fine_tuning_gemma-2b_instruct_for_summarization_assignment_version_jun14.ipynb) -  inspirational quotes from a tiny LM!!!
using all the good stuff from Hugging Face:
1. Load and Quantize the base Model (bitsandbytes) ğŸ”ğŸ“¦
2. Load and Prepare Data (datasets) ğŸ“‚ğŸ“Š
3. Fine Tuning Configuration (peft) âš™ï¸ğŸ› ï¸
4. Train the adapter Model (trl) ğŸ‹ï¸â€â™‚ï¸ğŸ¤–
5. Model Evaluation and Deployment (peft - merge and unload) ğŸ“ˆğŸš€)

using all the good stuff from Hugging Face:
1. Load and Quantize the base Model (bitsandbytes) ğŸ”ğŸ“¦
2. Load and Prepare Data (datasets) ğŸ“‚ğŸ“Š
3. Fine Tuning Configuration (peft) âš™ï¸ğŸ› ï¸
4. Train the adapter Model (trl) ğŸ‹ï¸â€â™‚ï¸ğŸ¤–
5. Model Evaluation and Deployment (peft - merge and unload) ğŸ“ˆğŸš€

### Setting Up
"""

base_model_name = "google/gemma-2b"
adapter_model_name = "dwb2023/gemma2b_quotes"

!pip install -qU bitsandbytes datasets accelerate loralib peft transformers trl

import torch
torch.cuda.is_available()

import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
import torch
import torch.nn as nn
import bitsandbytes as bnb
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig

"""## Load and Quatize the base Model (bitsandbytes)"""

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
)

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')

os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map='auto',
)

tokenizer = AutoTokenizer.from_pretrained(base_model_name)

"""### Model Architecture"""

print(base_model)

base_model.config

"""## Load and Prepare Data (datasets)"""

text = "Quote: Imagination is more"
device = "cuda:0"
inputs = tokenizer(text, return_tensors="pt").to(device)

outputs = base_model.generate(**inputs, max_new_tokens=32)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from datasets import Dataset, load_dataset

quote_dataset = load_dataset("Abirate/english_quotes")

quote_dataset

# print first record
print(quote_dataset["train"][0])

quote_dataset = quote_dataset.map(lambda samples: tokenizer(samples["quote"]), batched=True)

# create train test split with test_size=0.2
train_dataset = quote_dataset["train"].train_test_split(test_size=0.2)

# create test val split
train_dataset_test_valid = train_dataset["test"].train_test_split(test_size=0.5)

from datasets import DatasetDict

quote_dataset = DatasetDict({
    "train" : train_dataset["train"],
    "test" : train_dataset_test_valid["test"],
    "validation" : train_dataset_test_valid["train"]
})

# print quote_dataset showing train / test / validation
quote_dataset

"""## Fine Tuning Configuration (peft)"""

from peft import prepare_model_for_kbit_training
base_model.config.use_cache = False
base_model = prepare_model_for_kbit_training(base_model)

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

from peft import LoraConfig, get_peft_model

lora_r = 16
lora_dropout = 0.1
lora_alpha = 32

# construct our LoraConfig with the above hyperparameters
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
#    lora_dropout=lora_dropout,
    r=lora_r,
    bias="none",
    target_modules="all-linear",
    task_type="CAUSAL_LM"
)

model = get_peft_model(
    base_model,
    peft_config
)

### Model based on LoraConfig (peft_config)

print(model)

"""## Train the Adapter Model (trl)"""

from trl import SFTConfig

max_seq_length = 128
output_dir = "gemma2b_quotes"


def formatting_func(example):
    text = f"Quote: {example['quote'][0]}\nAuthor: {example['author'][0]}<eos>"
    return [text]

args=SFTConfig(
    output_dir = output_dir,
    max_steps=50,
    per_device_train_batch_size=1,
    warmup_steps=0.03,
    logging_steps=5,
    eval_strategy="steps",
    eval_steps=10,
    learning_rate=2e-4,
    lr_scheduler_type='constant',
    max_seq_length=max_seq_length,
    # packing=True,
    fp16=True,
    gradient_accumulation_steps=4,
    optim="paged_adamw_8bit"
)

# args = SFTConfig(
#   output_dir = "gemma2b_summarize",
#   #num_train_epochs=5,
#   max_steps = 100, # comment out this line if you want to train in epochs
#   per_device_train_batch_size = 1,
#   warmup_steps = 0.03,
#   logging_steps=10,
#   #evaluation_strategy="epoch",
#   eval_strategy="steps",
#   eval_steps=25, # comment out this line if you want to evaluate at the end of each epoch
#   learning_rate=2e-4,
#   lr_scheduler_type='constant',
#   max_seq_length=max_seq_length,
#   packing=True,
# )

from trl import SFTTrainer

trainer = SFTTrainer(
  model=model,
  peft_config=peft_config,
  tokenizer=tokenizer,
  formatting_func=formatting_func,
  args=args,
  train_dataset=quote_dataset["train"],
  eval_dataset=quote_dataset["validation"]
)

trainer.train()

trainer.push_to_hub(f"dwb2023/{output_dir}")

"""## Model Evaluation and Deployment (peft)"""

device = "cpu"

from peft import PeftConfig, PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(base_model_name)

"""### Use Adapters PEFT"""

model = AutoModelForCausalLM.from_pretrained(base_model_name)
model = PeftModel.from_pretrained(model, adapter_model_name)

text = "Quote: Imagination is"
# device = "cuda:0"
inputs = tokenizer(text, return_tensors="pt").to(device)

outputs = model.generate(**inputs, max_new_tokens=32)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

"""### Use Merged Model"""

model = AutoModelForCausalLM.from_pretrained(base_model_name)
model = PeftModel.from_pretrained(model, adapter_model_name)

model = model.merge_and_unload()
model.save_pretrained("merged_adapters")

text = "Quote: Imagination is"
#device = "cuda:0"
inputs = tokenizer(text, return_tensors="pt").to(device)

outputs = model.generate(**inputs, max_new_tokens=32)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

print(model)

text = "Quote: I have a dream"
#device = "cuda:0"
inputs = tokenizer(text, return_tensors="pt").to(device)

outputs = model.generate(**inputs, max_new_tokens=32)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

